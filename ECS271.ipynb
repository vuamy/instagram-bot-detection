{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 383
        },
        "id": "x2HKX3CliL5z",
        "outputId": "faf57780-7108-458a-dcc3-cebbe99af11c"
      },
      "outputs": [],
      "source": [
        "# all imported libraries\n",
        "import os\n",
        "os.environ['TF_USE_LEGACY_KERAS'] = '1'\n",
        "import numpy\n",
        "import pandas as pd\n",
        "import matplotlib\n",
        "import csv\n",
        "import os\n",
        "import torch\n",
        "import torchvision\n",
        "from torchvision import transforms\n",
        "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import classification_report\n",
        "from transformers import AutoTokenizer, TFAutoModelForSequenceClassification, pipeline, ResNetForImageClassification, AutoImageProcessor, TrainingArguments, Trainer\n",
        "from datasets import Dataset, DatasetDict, load_metric, Image\n",
        "from tensorflow.keras.models import Sequential, Model\n",
        "from tensorflow.keras.layers import BatchNormalization, Conv2D, MaxPooling2D, Activation, Dropout, Dense, Flatten, Input, concatenate, Embedding, Conv1D, GlobalMaxPooling1D, TextVectorization\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "import PIL\n",
        "from PIL import Image as im\n",
        "from PIL import ImageOps as imops\n",
        "import shutil"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iu_BrqkbA3-m",
        "outputId": "cb1dba8e-083b-46e0-d124-5fa4f205c962"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "^C\n"
          ]
        }
      ],
      "source": [
        "!grep Instagram_Username_Dataset_Updated.csv"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "reUAHWJ_j9o3"
      },
      "source": [
        "####Data to Array"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 216
        },
        "id": "Mg5MQ8V0kAhw",
        "outputId": "10f6daca-8546-40e3-df5e-22088abac27c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Filling in Usernames\n"
          ]
        }
      ],
      "source": [
        "usernames = []\n",
        "followers = []\n",
        "following = []\n",
        "biography = []\n",
        "profile_picture = []\n",
        "post_image = []\n",
        "post_caption = []\n",
        "labels = []\n",
        "count = 0\n",
        "\n",
        "__file__ = \"ECS271.ipynb\"\n",
        "script_dir = os.path.dirname(__file__)\n",
        "script_dir = os.path.join(script_dir, \"data\")\n",
        "\n",
        "print(\"Filling in Usernames\")\n",
        "# fill in all usernames and their label\n",
        "with open('./Instagram_Username_Dataset_Updated.csv', encoding='utf-8') as fp:\n",
        "    csvreader = csv.reader(fp)\n",
        "    for name in csvreader:\n",
        "        file_path = os.path.join(script_dir, name[0])\n",
        "        follow_path = os.path.join(file_path, name[0] + \"_follower_info.txt\")\n",
        "        if (os.path.isfile(follow_path) == False):\n",
        "            continue\n",
        "        usernames.append(name[0])\n",
        "        labels.append(int(name[1]))\n",
        "\n",
        "# loop through every user data file=\n",
        "for name in usernames:\n",
        "    file_path = os.path.join(script_dir, name)\n",
        "    follow_path = os.path.join(file_path, name + \"_follower_info.txt\")\n",
        "    # add numerical data\n",
        "    try:\n",
        "        infile = open(follow_path, \"r\")\n",
        "        temp = []\n",
        "        for line in infile:\n",
        "            line=line.strip()\n",
        "            line = int(line)\n",
        "            temp.append(line)\n",
        "        infile.close()\n",
        "    except:\n",
        "        temp = [0, 0]\n",
        "    followers.append(int(temp[0]))\n",
        "    following.append(int(temp[1]))\n",
        "\n",
        "    # add biography data\n",
        "    bio_path = os.path.join(file_path, name + \"_biography.txt\")\n",
        "    temp = ''\n",
        "    try:\n",
        "        infile = open(bio_path, \"r\")\n",
        "        for line in infile:\n",
        "            line = line.strip()\n",
        "            temp = temp + \" \" + line\n",
        "    except:\n",
        "        temp = ''\n",
        "    biography.append(temp)\n",
        "    infile.close()\n",
        "\n",
        "    # add profile image data\n",
        "    contain_profile = False\n",
        "    try:\n",
        "        for images in os.listdir(file_path):\n",
        "            if (images.endswith(\"_profile_pic.jpg\")):\n",
        "                profile_picture.append(\"./data/\" + name + \"/\" + images)\n",
        "                contain_profile = True\n",
        "    except:\n",
        "        contain_profile = False\n",
        "    if contain_profile == False:\n",
        "        profile_picture.append('./data/2018-11-21_19-35-46_UTC_profile_pic.jpg') # load in default pfp if missing\n",
        "\n",
        "    # add post image data\n",
        "    post_image_path = os.path.join(file_path, name + \"_images\")\n",
        "    # add post image data\n",
        "    temp = []\n",
        "    contain_posts = False\n",
        "    has_four = False\n",
        "    count = 0\n",
        "    try:\n",
        "        for images in os.listdir(post_image_path):\n",
        "            if (images.endswith(\".jpg\")):\n",
        "                temp.append(post_image_path + \"/\" + images)\n",
        "                contain_posts = True\n",
        "                if count == 4:\n",
        "                    has_four = True\n",
        "                count += 1\n",
        "    except:\n",
        "        contain_posts = False\n",
        "        temp = []\n",
        "    if contain_posts == False:\n",
        "        dst = './data/2018-11-21_19-35-46_UTC_profile_pic.jpg'\n",
        "        temp.append(dst)\n",
        "        count += 1\n",
        "    if has_four == False:\n",
        "        while count < 4:\n",
        "            dst = './data/2018-11-21_19-35-46_UTC_profile_pic.jpg'\n",
        "            temp.append(dst)\n",
        "            count += 1\n",
        "\n",
        "    post_image.append(temp)\n",
        "\n",
        "    # add caption image data\n",
        "    caption_path = os.path.join(file_path, name + \"_captions.txt\")\n",
        "    temp = []\n",
        "    try:\n",
        "        infile = open(caption_path, \"r\")\n",
        "        for line in infile:\n",
        "            line = line.strip()\n",
        "            temp.append(line)\n",
        "    except:\n",
        "        temp = []\n",
        "    post_caption.append(temp)\n",
        "    infile.close()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "5aziahyMu-SW"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "           username  followers  following  \\\n",
            "0         r.zsyifaa        510        372   \n",
            "1    shabitanurch31        232        242   \n",
            "2         tatiazulp         25         31   \n",
            "3             00td4        261        316   \n",
            "4         lariybaby        613        585   \n",
            "..              ...        ...        ...   \n",
            "162        cppijazc          9          8   \n",
            "163       kcqaw2024         16          8   \n",
            "164       payxc2024         17          8   \n",
            "165         ewsfkrd          9          8   \n",
            "166         phoovow         11          8   \n",
            "\n",
            "                                             biography  \\\n",
            "0                                            Bismillah   \n",
            "1                                                        \n",
            "2     🎀 | romanticizing life ꒰⁠⑅⁠ᵕ⁠༚⁠ᵕ⁠꒱⁠˖⁠♡ @xsven...   \n",
            "3                                                        \n",
            "4                       Asuna🐾 • 𝑒𝓈𝒸𝒶𝓅𝑒 𝓉𝒽𝑒 𝑜𝓇𝒹𝒾𝓃𝒶𝓇𝓎 •   \n",
            "..                                                 ...   \n",
            "162                                                      \n",
            "163                                                      \n",
            "164                                                      \n",
            "165                                                      \n",
            "166                                                      \n",
            "\n",
            "                                       profile_picture  \\\n",
            "0    ./data/r.zsyifaa/2024-04-14_15-08-16_UTC_profi...   \n",
            "1    ./data/shabitanurch31/2024-05-04_13-12-01_UTC_...   \n",
            "2    ./data/tatiazulp/2024-02-05_03-39-44_UTC_profi...   \n",
            "3    ./data/00td4/2023-07-30_22-25-56_UTC_profile_p...   \n",
            "4    ./data/lariybaby/2024-04-22_01-51-40_UTC_profi...   \n",
            "..                                                 ...   \n",
            "162  ./data/bbukico/2024-04-08_18-00-15_UTC_profile...   \n",
            "163  ./data/cppijazc/2024-04-08_18-00-39_UTC_profil...   \n",
            "164  ./data/kcqaw2024/2024-04-08_17-49-07_UTC_profi...   \n",
            "165  ./data/payxc2024/2024-04-08_17-48-54_UTC_profi...   \n",
            "166  ./data/ewsfkrd/2024-04-08_17-48-42_UTC_profile...   \n",
            "\n",
            "                                           post_images  \\\n",
            "0    [data\\r.zsyifaa\\r.zsyifaa_images/r.zsyifaa_ima...   \n",
            "1    [data\\shabitanurch31\\shabitanurch31_images/sha...   \n",
            "2    [data\\tatiazulp\\tatiazulp_images/tatiazulp_ima...   \n",
            "3    [data\\00td4\\00td4_images/00td4_image_1.jpg, da...   \n",
            "4    [data\\lariybaby\\lariybaby_images/lariybaby_ima...   \n",
            "..                                                 ...   \n",
            "162  [./data/2018-11-21_19-35-46_UTC_profile_pic.jp...   \n",
            "163  [./data/2018-11-21_19-35-46_UTC_profile_pic.jp...   \n",
            "164  [./data/2018-11-21_19-35-46_UTC_profile_pic.jp...   \n",
            "165  [./data/2018-11-21_19-35-46_UTC_profile_pic.jp...   \n",
            "166  [./data/2018-11-21_19-35-46_UTC_profile_pic.jp...   \n",
            "\n",
            "                                         post_captions  label  \n",
            "0    [Dan Pada akhirnya kita akan sampai pada titik...      0  \n",
            "1    [Alhamdulillah..., Nikmat mana lagi yg kau dus...      0  \n",
            "2    [🩷, Yeah my boyfriend is pretty cul 🤓, Think I...      0  \n",
            "3     [Drippy😌, Love you , always🥰, #explorepage, ☠️🥴]      0  \n",
            "4    [🍓, Esse dia foi d+🛟🌳🪨, 💚, 20/04/2024✨️, 🤍, Me...      0  \n",
            "..                                                 ...    ...  \n",
            "162                                                 []      1  \n",
            "163                                                 []      1  \n",
            "164                                                 []      1  \n",
            "165                                                 []      1  \n",
            "166                                                 []      1  \n",
            "\n",
            "[167 rows x 8 columns]\n"
          ]
        }
      ],
      "source": [
        "# append all to train data\n",
        "train_data = []\n",
        "pfp_train_init = []\n",
        "for x in range (0,len(usernames)):\n",
        "  temp = [usernames[x], followers[x], following[x], biography[x], profile_picture[x], post_image[x], post_caption[x], labels[x]]\n",
        "  train_data.append(temp)\n",
        "  #temp = [profile_picture[x], labels[x]]\n",
        "  #pfp_train_init.append(temp)\n",
        "\n",
        "# train_data[x][0] = username\n",
        "# train_data[x][1] = number of followers\n",
        "# train_data[x][2] = number of users following\n",
        "# train_data[x][3] = biography\n",
        "# train_data[x][4] = string of profile picture\n",
        "# train_data[x][5] = array of strings of post images\n",
        "# train_data[x][6] = array of post captions\n",
        "# train_data[x][7] = binary classification\n",
        "\n",
        "# convert to df\n",
        "df = pd.DataFrame(train_data)\n",
        "#df_pfp = pd.DataFrame(pfp_train_init.append)\n",
        "df.columns = [\"username\", \"followers\", \"following\", \"biography\", \"profile_picture\", \"post_images\", \"post_captions\", \"label\"]\n",
        "#df_pfp.columns = [\"profile_picture\",\"label\"]\n",
        "\n",
        "print(df)\n",
        "\n",
        "# split training and testing data\n",
        "x = df.drop(\"label\", axis=1)\n",
        "y = df[\"label\"]\n",
        "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.2, random_state=42)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bcE6DqkThsbG"
      },
      "source": [
        "####Numerical Classification\n",
        "\n",
        "*   Number of Followers\n",
        "*   Number of Users Following\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "Yl4gau_nh04g"
      },
      "outputs": [],
      "source": [
        "# function to process numerical attributes\n",
        "def process_num_attributes(train, test):\n",
        "\tcontinuous = [\"followers\", \"following\"]\n",
        "\tcs = MinMaxScaler()\n",
        "\ttrain_cont = cs.fit_transform(train[continuous])\n",
        "\ttest_cont = cs.transform(test[continuous])\n",
        "\treturn train_cont, test_cont\n",
        "\n",
        "# apply function on data\n",
        "x_num_train, x_num_test = process_num_attributes(x_train, x_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "Zb42rSMUG6Pm"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From c:\\Users\\Aaron Nguyen\\anaconda3\\Lib\\site-packages\\tf_keras\\src\\backend.py:1400: The name tf.executing_eagerly_outside_functions is deprecated. Please use tf.compat.v1.executing_eagerly_outside_functions instead.\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# mlp created with keras\n",
        "def create_mlp(dim, regress=False):\n",
        "    input_layer = Input(shape=(dim,))\n",
        "    # add dense layers\n",
        "    layer = Dense(8, activation=\"relu\")(input_layer)\n",
        "    layer = Dense(4, activation=\"relu\")(layer)\n",
        "    if regress:\n",
        "        layer = Dense(1, activation=\"linear\")(layer)\n",
        "    model = Model(inputs=input_layer, outputs=layer)\n",
        "    return model\n",
        "\n",
        "# create/compile model\n",
        "num_input = Input(shape=(x_num_train.shape[1],))\n",
        "num_test = Input(shape=(x_num_test.shape[1],))\n",
        "num_model = create_mlp(2, regress=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8zInhVqfhdqX"
      },
      "source": [
        "####Image Classification:\n",
        "\n",
        "*   Profile Picture\n",
        "*   Post Images\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "CMPEJtvbhOUN"
      },
      "outputs": [],
      "source": [
        "#tensorflow pfp_classifier set-up\n",
        "\n",
        "pfps = []\n",
        "post_pics = []\n",
        "lbls = []\n",
        "for x in range (0,len(usernames)):\n",
        "    if profile_picture[x] != '':\n",
        "        pfps.append(profile_picture[x])\n",
        "        lbls.append(labels[x])\n",
        "    else:\n",
        "        print(usernames[x])\n",
        "\n",
        "    im_temp = []\n",
        "    for i in range (4):\n",
        "        if(len(post_image[x]) > i):\n",
        "            img = im.open(post_image[x][i])\n",
        "            img = img.resize((512,512), im.LANCZOS)\n",
        "            im_temp.append(img)\n",
        "\n",
        "    im12 = im.new('RGB', (im_temp[0].width + im_temp[1].width, im_temp[0].height))\n",
        "    im34 = im.new('RGB', (im_temp[2].width + im_temp[3].width, im_temp[2].height))\n",
        "    im12.paste(im_temp[0], (0, 0))\n",
        "    im12.paste(im_temp[1], (im_temp[0].width, 0))\n",
        "    im34.paste(im_temp[2], (0, 0))\n",
        "    im34.paste(im_temp[3], (im_temp[2].width, 0))\n",
        "\n",
        "    new_image = im.new('RGB', (im12.width, im12.height + im34.height))\n",
        "    new_image.paste(im12, (0, 0))\n",
        "    new_image.paste(im34, (0, im12.height))\n",
        "\n",
        "    new_image = new_image.resize((224,224), im.LANCZOS)\n",
        "    img_path = script_dir+\"/\" + usernames[x] + \"/joined_img.jpg\"\n",
        "    new_image.save(img_path)\n",
        "    post_pics.append(img_path)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "W5lclNMamlxz"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "a87ff12333224ecba8247970be6f1611",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Map:   0%|          | 0/167 [00:00<?, ? examples/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "./data/r.zsyifaa/2024-04-14_15-08-16_UTC_profile_pic.jpg\n",
            "./data/shabitanurch31/2024-05-04_13-12-01_UTC_profile_pic.jpg\n",
            "./data/tatiazulp/2024-02-05_03-39-44_UTC_profile_pic.jpg\n",
            "./data/00td4/2023-07-30_22-25-56_UTC_profile_pic.jpg\n",
            "./data/lariybaby/2024-04-22_01-51-40_UTC_profile_pic.jpg\n",
            "./data/neseekilic/2024-04-25_20-42-22_UTC_profile_pic.jpg\n",
            "./data/gacha_editsfork/2024-05-01_15-48-31_UTC_profile_pic.jpg\n",
            "./data/_rawann10_/2024-04-19_11-07-20_UTC_profile_pic.jpg\n",
            "./data/honeysoymilktea/2024-05-06_22-52-37_UTC_profile_pic.jpg\n",
            "./data/rarelykind/2024-02-28_21-46-22_UTC_profile_pic.jpg\n",
            "./data/nsashleyx/2024-05-11_23-44-28_UTC_profile_pic.jpg\n",
            "./data/ms_grayned/2022-05-29_20-30-31_UTC_profile_pic.jpg\n",
            "./data/ilovefashion719/2024-05-01_18-27-54_UTC_profile_pic.jpg\n",
            "./data/5star.iha/2018-11-21_19-35-46_UTC_profile_pic.jpg\n",
            "./data/ranniiyyaa/2024-03-29_14-32-57_UTC_profile_pic.jpg\n",
            "./data/kevon.365/2023-06-25_03-56-54_UTC_profile_pic.jpg\n",
            "./data/hennbro1/2014-11-26_02-59-16_UTC_profile_pic.jpg\n",
            "./data/inoka_ath/2015-07-01_02-48-55_UTC_profile_pic.jpg\n",
            "./data/verra_uliani/2024-05-05_13-40-59_UTC_profile_pic.jpg\n",
            "./data/rebeccasugar/2023-09-06_01-30-03_UTC_profile_pic.jpg\n",
            "./data/danaterrace/2023-12-05_15-28-23_UTC_profile_pic.jpg\n",
            "./data/baleen.sound/2024-04-12_22-03-26_UTC_profile_pic.jpg\n",
            "./data/xoxomobxoxo/2024-03-27_03-40-44_UTC_profile_pic.jpg\n",
            "./data/sthamtp/2024-03-15_16-31-22_UTC_profile_pic.jpg\n",
            "./data/gwagwa._/2024-04-20_22-57-12_UTC_profile_pic.jpg\n",
            "./data/hayeyashi/2019-10-22_00-41-50_UTC_profile_pic.jpg\n",
            "./data/alayna.smith120/2023-07-26_04-16-44_UTC_profile_pic.jpg\n",
            "./data/fakedgold/2023-10-20_12-38-35_UTC_profile_pic.jpg\n",
            "./data/aw_yes/2024-01-07_04-20-24_UTC_profile_pic.jpg\n",
            "./data/cat.finay/2022-08-16_20-51-25_UTC_profile_pic.jpg\n",
            "./data/theicon5/2020-05-21_03-18-41_UTC_profile_pic.jpg\n",
            "./data/elijah__self/2022-12-12_18-27-22_UTC_profile_pic.jpg\n",
            "./data/ethand1117/2022-01-22_20-08-18_UTC_profile_pic.jpg\n",
            "./data/zled.art/2023-11-13_02-46-49_UTC_profile_pic.jpg\n",
            "./data/its_toast22/2023-10-28_05-59-21_UTC_profile_pic.jpg\n",
            "./data/oatmilkpapi/2024-03-27_12-27-04_UTC_profile_pic.jpg\n",
            "./data/tristiancook/2023-11-11_14-59-02_UTC_profile_pic.jpg\n",
            "./data/not_them/2018-12-20_20-19-16_UTC_profile_pic.jpg\n",
            "./data/ultralight_memories/2024-01-01_03-48-51_UTC_profile_pic.jpg\n",
            "./data/ljosfarii/2021-04-29_13-10-58_UTC_profile_pic.jpg\n",
            "./data/nutty_twitch/2020-02-17_05-01-28_UTC_profile_pic.jpg\n",
            "./data/solielxeno/2023-11-24_15-37-53_UTC_profile_pic.jpg\n",
            "./data/curtwilde/2024-01-13_06-36-24_UTC_profile_pic.jpg\n",
            "./data/buntykumar8244/2024-05-07_04-00-48_UTC_profile_pic.jpg\n",
            "./data/aimrhodora/2024-04-15_00-17-30_UTC_profile_pic.jpg\n",
            "./data/macsadbh/2022-11-05_12-38-09_UTC_profile_pic.jpg\n",
            "./data/kaitlynjb2006_/2023-05-13_12-07-53_UTC_profile_pic.jpg\n",
            "./data/silambarasu/2019-08-06_14-57-53_UTC_profile_pic.jpg\n",
            "./data/peyadele7/2022-06-22_02-56-09_UTC_profile_pic.jpg\n",
            "./data/jeff_jitsu/2023-04-10_02-22-38_UTC_profile_pic.jpg\n",
            "./data/dontcryoverspilling/2024-05-16_07-45-27_UTC_profile_pic.jpg\n",
            "./data/dontcryoverspilling/2024-05-16_17-35-02_UTC_profile_pic.jpg\n",
            "./data/lildrummerdude/2021-10-28_10-09-23_UTC_profile_pic.jpg\n",
            "./data/_tiny_truly_/2024-04-10_02-36-31_UTC_profile_pic.jpg\n",
            "./data/ynj_frdm76/2021-09-14_06-27-00_UTC_profile_pic.jpg\n",
            "./data/minthuta_297/2020-07-07_15-47-25_UTC_profile_pic.jpg\n",
            "./data/oscarandrade305/2024-05-01_02-06-21_UTC_profile_pic.jpg\n",
            "./data/phia.henry/2022-09-14_14-11-36_UTC_profile_pic.jpg\n",
            "./data/larionmusic/2024-03-03_07-04-35_UTC_profile_pic.jpg\n",
            "./data/mel_ody42/2023-11-29_10-06-17_UTC_profile_pic.jpg\n",
            "./data/omgdarbiomg/2015-08-27_08-09-56_UTC_profile_pic.jpg\n",
            "./data/gaia_filia/2023-03-26_17-38-40_UTC_profile_pic.jpg\n",
            "./data/miracleboymimimi/2024-05-11_23-09-02_UTC_profile_pic.jpg\n",
            "./data/arthurocampo_/2022-05-11_14-59-44_UTC_profile_pic.jpg\n",
            "./data/ningen_011/2022-09-20_12-26-02_UTC_profile_pic.jpg\n",
            "./data/_maryamore/2023-12-27_05-52-16_UTC_profile_pic.jpg\n",
            "./data/doncellalejis/2024-04-28_05-44-47_UTC_profile_pic.jpg\n",
            "./data/bexil.jbell/2024-04-21_19-10-17_UTC_profile_pic.jpg\n",
            "./data/emorie.m/2024-05-03_23-31-35_UTC_profile_pic.jpg\n",
            "./data/jenna_segal/2022-12-07_22-27-58_UTC_profile_pic.jpg\n",
            "./data/alanajonick/2023-07-02_02-37-19_UTC_profile_pic.jpg\n",
            "./data/nekomilkie/2024-04-09_17-12-56_UTC_profile_pic.jpg\n",
            "./data/ashley.laroc/2024-03-11_00-57-24_UTC_profile_pic.jpg\n",
            "./data/trin.spadaro/2024-05-15_16-05-03_UTC_profile_pic.jpg\n",
            "./data/alejo._fajardo/2024-04-30_00-44-30_UTC_profile_pic.jpg\n",
            "./data/kim.thefoodie/2023-11-24_03-07-28_UTC_profile_pic.jpg\n",
            "./data/grillospickles/2021-11-04_18-08-50_UTC_profile_pic.jpg\n",
            "./data/kalanididit/2018-12-13_07-27-55_UTC_profile_pic.jpg\n",
            "./data/beeladykim/2020-10-05_18-32-58_UTC_profile_pic.jpg\n",
            "./data/lukxstn/2023-11-06_05-51-37_UTC_profile_pic.jpg\n",
            "./data/claus_de_zwaan/2023-11-27_15-43-30_UTC_profile_pic.jpg\n",
            "./data/stoccafisso_design/2022-03-09_08-35-43_UTC_profile_pic.jpg\n",
            "./data/itschuzzy/2024-04-04_02-48-08_UTC_profile_pic.jpg\n",
            "./data/richardromanashton/2023-07-19_03-19-58_UTC_profile_pic.jpg\n",
            "./data/shivam10_7/2023-01-01_20-14-20_UTC_profile_pic.jpg\n",
            "./data/jabonthegringo/2024-03-26_21-05-30_UTC_profile_pic.jpg\n",
            "./data/brandongouveia/2024-01-22_03-34-45_UTC_profile_pic.jpg\n",
            "./data/jasminepak/2019-09-29_05-35-21_UTC_profile_pic.jpg\n",
            "./data/bridget__widget/2024-02-02_01-29-30_UTC_profile_pic.jpg\n",
            "./data/fbuksauce/2023-07-25_02-26-42_UTC_profile_pic.jpg\n",
            "./data/mattwener/2024-02-06_03-35-06_UTC_profile_pic.jpg\n",
            "./data/azizo_ahmed_/2024-04-11_08-20-43_UTC_profile_pic.jpg\n",
            "./data/sa.deg694/2021-12-29_12-00-00_UTC_profile_pic.jpg\n",
            "./data/wwwww20142020/2021-09-18_00-05-27_UTC_profile_pic.jpg\n",
            "./data/cdppdc6300/2018-11-21_19-35-46_UTC_profile_pic.jpg\n",
            "./data/kornbredteev/2024-02-27_18-17-57_UTC_profile_pic.jpg\n",
            "./data/sazwan_95/2018-11-21_19-35-46_UTC_profile_pic.jpg\n",
            "./data/mulia.budy/2018-11-21_19-35-46_UTC_profile_pic.jpg\n",
            "./data/staiy55/2018-11-21_19-35-46_UTC_profile_pic.jpg\n",
            "./data/namasayava/2024-05-12_22-07-44_UTC_profile_pic.jpg\n",
            "./data/dinsy99/2022-11-08_10-25-21_UTC_profile_pic.jpg\n",
            "./data/setyanto2706/2018-11-21_19-35-46_UTC_profile_pic.jpg\n",
            "./data/milfs.indian/2018-11-21_19-35-46_UTC_profile_pic.jpg\n",
            "./data/hot_dilf_feet/2018-11-21_19-35-46_UTC_profile_pic.jpg\n",
            "./data/bigd.ick4623/2018-11-21_19-35-46_UTC_profile_pic.jpg\n",
            "./data/bigd.ick4629/2018-11-21_19-35-46_UTC_profile_pic.jpg\n",
            "./data/lamaer508/2018-11-21_19-35-46_UTC_profile_pic.jpg\n",
            "./data/haruy.r/2018-11-21_19-35-46_UTC_profile_pic.jpg\n",
            "./data/antonellasofiamoreno/2024-04-27_02-20-48_UTC_profile_pic.jpg\n",
            "./data/hungredy_3/2024-02-14_22-11-46_UTC_profile_pic.jpg\n",
            "./data/23.1osohee/2024-05-17_14-09-28_UTC_profile_pic.jpg\n",
            "./data/krishnashyammishra28/2018-11-21_19-35-46_UTC_profile_pic.jpg\n",
            "./data/likeeebutter/2018-11-21_19-35-46_UTC_profile_pic.jpg\n",
            "./data/lyingisland/2018-11-21_19-35-46_UTC_profile_pic.jpg\n",
            "./data/lover.feedzzz/2018-11-21_19-35-46_UTC_profile_pic.jpg\n",
            "./data/follow_tanveer.gilll/2018-11-21_19-35-46_UTC_profile_pic.jpg\n",
            "./data/2018-11-21_19-35-46_UTC_profile_pic.jpg\n",
            "./data/m.a.d.d.i.2005/2018-11-21_19-35-46_UTC_profile_pic.jpg\n",
            "./data/big.nng/2018-11-21_19-35-46_UTC_profile_pic.jpg\n",
            "./data/ch.ristian5363/2019-04-16_02-16-12_UTC_profile_pic.jpg\n",
            "./data/acjaiajsksl/2018-11-21_19-35-46_UTC_profile_pic.jpg\n",
            "./data/9062.sani/2024-04-29_02-21-21_UTC_profile_pic.jpg\n",
            "./data/irah180605gmail.com8/2023-10-06_08-27-24_UTC_profile_pic.jpg\n",
            "./data/nrmynti0824/2018-11-21_19-35-46_UTC_profile_pic.jpg\n",
            "./data/johnjames13051990/2018-11-21_19-35-46_UTC_profile_pic.jpg\n",
            "./data/acjaiajsksl/2018-11-21_19-35-46_UTC_profile_pic.jpg\n",
            "./data/his.mayanti/2024-03-28_11-51-24_UTC_profile_pic.jpg\n",
            "./data/aussy_bakkes/2024-01-15_19-49-11_UTC_profile_pic.jpg\n",
            "./data/yoneiby0__0/2018-11-21_19-35-46_UTC_profile_pic.jpg\n",
            "./data/sivha.sing/2024-01-30_14-46-48_UTC_profile_pic.jpg\n",
            "./data/5julissa_300_juliana_vx/2024-01-26_15-48-01_UTC_profile_pic.jpg\n",
            "./data/mindfulness.kite/2023-03-31_06-44-59_UTC_profile_pic.jpg\n",
            "./data/syaripah_we/2024-05-13_09-14-29_UTC_profile_pic.jpg\n",
            "./data/johnk939620/2018-11-21_19-35-46_UTC_profile_pic.jpg\n",
            "./data/dinidinii40/2024-01-08_05-43-03_UTC_profile_pic.jpg\n",
            "./data/marwotoarlan/2024-01-05_02-49-37_UTC_profile_pic.jpg\n",
            "./data/jozsi0918273645/2017-05-10_16-15-29_UTC_profile_pic.jpg\n",
            "./data/alexis_chose/2018-11-21_19-35-46_UTC_profile_pic.jpg\n",
            "./data/action_ou_veriter_hot/2022-06-28_19-50-18_UTC_profile_pic.jpg\n",
            "./data/ambreychaney_/2024-01-05_11-19-54_UTC_profile_pic.jpg\n",
            "./data/neymarjunior2.0vibes/2024-05-08_14-46-36_UTC_profile_pic.jpg\n",
            "./data/liza_lyz245/2023-11-20_07-30-09_UTC_profile_pic.jpg\n",
            "./data/hofdefender/2018-11-21_19-35-46_UTC_profile_pic.jpg\n",
            "./data/georgepeters969/2023-11-06_06-51-41_UTC_profile_pic.jpg\n",
            "./data/onlyn351/2018-11-21_19-35-46_UTC_profile_pic.jpg\n",
            "./data/nkheleuf/2023-12-11_04-33-42_UTC_profile_pic.jpg\n",
            "./data/ilbalnugraha/2023-09-20_12-24-15_UTC_profile_pic.jpg\n",
            "./data/ritvakorhonen2023/2023-10-25_08-42-37_UTC_profile_pic.jpg\n",
            "./data/ayang_angga84/2023-11-03_03-46-55_UTC_profile_pic.jpg\n",
            "./data/peju.ang4080/2024-05-05_11-35-54_UTC_profile_pic.jpg\n",
            "./data/chell6194/2024-02-03_06-34-54_UTC_profile_pic.jpg\n",
            "./data/awanputra902gmail.com7/2023-08-22_19-22-45_UTC_profile_pic.jpg\n",
            "./data/87benjohnson/2018-11-21_19-35-46_UTC_profile_pic.jpg\n",
            "./data/jokersson7/2023-08-29_17-15-33_UTC_profile_pic.jpg\n",
            "./data/slipsnfallslive/2023-12-11_03-48-19_UTC_profile_pic.jpg\n",
            "./data/sirach51k51/2018-11-21_19-35-46_UTC_profile_pic.jpg\n",
            "./data/schei_dflschiedsrichter/2018-11-21_19-35-46_UTC_profile_pic.jpg\n",
            "./data/shell.o.kk26/2024-04-12_01-55-25_UTC_profile_pic.jpg\n",
            "./data/callmebabe8889/2023-11-14_15-10-21_UTC_profile_pic.jpg\n",
            "./data/yxgfdq/2024-04-08_17-38-11_UTC_profile_pic.jpg\n",
            "./data/meejks/2024-04-08_17-37-59_UTC_profile_pic.jpg\n",
            "./data/sfpfsr/2018-11-21_19-35-46_UTC_profile_pic.jpg\n",
            "./data/bbukico/2024-04-08_18-00-15_UTC_profile_pic.jpg\n",
            "./data/cppijazc/2024-04-08_18-00-39_UTC_profile_pic.jpg\n",
            "./data/kcqaw2024/2024-04-08_17-49-07_UTC_profile_pic.jpg\n",
            "./data/payxc2024/2024-04-08_17-48-54_UTC_profile_pic.jpg\n",
            "./data/ewsfkrd/2024-04-08_17-48-42_UTC_profile_pic.jpg\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "9d8b6e0fc5cd4d0d9378cf76d9da6c9c",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Map:   0%|          | 0/167 [00:00<?, ? examples/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "data/r.zsyifaa/joined_img.jpg\n",
            "data/shabitanurch31/joined_img.jpg\n",
            "data/tatiazulp/joined_img.jpg\n",
            "data/00td4/joined_img.jpg\n",
            "data/lariybaby/joined_img.jpg\n",
            "data/neseekilic/joined_img.jpg\n",
            "data/gacha_editsfork/joined_img.jpg\n",
            "data/_rawann10_/joined_img.jpg\n",
            "data/honeysoymilktea/joined_img.jpg\n",
            "data/rarelykind/joined_img.jpg\n",
            "data/nsashleyx/joined_img.jpg\n",
            "data/ms_grayned/joined_img.jpg\n",
            "data/ilovefashion719/joined_img.jpg\n",
            "data/5star.iha/joined_img.jpg\n",
            "data/ranniiyyaa/joined_img.jpg\n",
            "data/kevon.365/joined_img.jpg\n",
            "data/hennbro1/joined_img.jpg\n",
            "data/inoka_ath/joined_img.jpg\n",
            "data/verra_uliani/joined_img.jpg\n",
            "data/rebeccasugar/joined_img.jpg\n",
            "data/danaterrace/joined_img.jpg\n",
            "data/baleen.sound/joined_img.jpg\n",
            "data/xoxomobxoxo/joined_img.jpg\n",
            "data/sthamtp/joined_img.jpg\n",
            "data/gwagwa._/joined_img.jpg\n",
            "data/hayeyashi/joined_img.jpg\n",
            "data/alayna.smith120/joined_img.jpg\n",
            "data/fakedgold/joined_img.jpg\n",
            "data/aw_yes/joined_img.jpg\n",
            "data/cat.finay/joined_img.jpg\n",
            "data/theicon5/joined_img.jpg\n",
            "data/elijah__self/joined_img.jpg\n",
            "data/ethand1117/joined_img.jpg\n",
            "data/zled.art/joined_img.jpg\n",
            "data/its_toast22/joined_img.jpg\n",
            "data/oatmilkpapi/joined_img.jpg\n",
            "data/tristiancook/joined_img.jpg\n",
            "data/not_them/joined_img.jpg\n",
            "data/ultralight_memories/joined_img.jpg\n",
            "data/ljosfarii/joined_img.jpg\n",
            "data/nutty_twitch/joined_img.jpg\n",
            "data/solielxeno/joined_img.jpg\n",
            "data/curtwilde/joined_img.jpg\n",
            "data/buntykumar8244/joined_img.jpg\n",
            "data/aimrhodora/joined_img.jpg\n",
            "data/macsadbh/joined_img.jpg\n",
            "data/kaitlynjb2006_/joined_img.jpg\n",
            "data/silambarasu/joined_img.jpg\n",
            "data/peyadele7/joined_img.jpg\n",
            "data/jeff_jitsu/joined_img.jpg\n",
            "data/dontcryoverspilling/joined_img.jpg\n",
            "data/lildrummerdude/joined_img.jpg\n",
            "data/_tiny_truly_/joined_img.jpg\n",
            "data/ynj_frdm76/joined_img.jpg\n",
            "data/minthuta_297/joined_img.jpg\n",
            "data/oscarandrade305/joined_img.jpg\n",
            "data/phia.henry/joined_img.jpg\n",
            "data/larionmusic/joined_img.jpg\n",
            "data/mel_ody42/joined_img.jpg\n",
            "data/omgdarbiomg/joined_img.jpg\n",
            "data/gaia_filia/joined_img.jpg\n",
            "data/miracleboymimimi/joined_img.jpg\n",
            "data/arthurocampo_/joined_img.jpg\n",
            "data/ningen_011/joined_img.jpg\n",
            "data/_maryamore/joined_img.jpg\n",
            "data/doncellalejis/joined_img.jpg\n",
            "data/bexil.jbell/joined_img.jpg\n",
            "data/emorie.m/joined_img.jpg\n",
            "data/jenna_segal/joined_img.jpg\n",
            "data/alanajonick/joined_img.jpg\n",
            "data/nekomilkie/joined_img.jpg\n",
            "data/ashley.laroc/joined_img.jpg\n",
            "data/trin.spadaro/joined_img.jpg\n",
            "data/alejo._fajardo/joined_img.jpg\n",
            "data/kim.thefoodie/joined_img.jpg\n",
            "data/grillospickles/joined_img.jpg\n",
            "data/kalanididit/joined_img.jpg\n",
            "data/beeladykim/joined_img.jpg\n",
            "data/lukxstn/joined_img.jpg\n",
            "data/claus_de_zwaan/joined_img.jpg\n",
            "data/stoccafisso_design/joined_img.jpg\n",
            "data/itschuzzy/joined_img.jpg\n",
            "data/richardromanashton/joined_img.jpg\n",
            "data/shivam10_7/joined_img.jpg\n",
            "data/jabonthegringo/joined_img.jpg\n",
            "data/brandongouveia/joined_img.jpg\n",
            "data/jasminepak/joined_img.jpg\n",
            "data/bridget__widget/joined_img.jpg\n",
            "data/fbuksauce/joined_img.jpg\n",
            "data/mattwener/joined_img.jpg\n",
            "data/azizo_ahmed_/joined_img.jpg\n",
            "data/sa.deg694/joined_img.jpg\n",
            "data/wwwww20142020/joined_img.jpg\n",
            "data/cdppdc6300/joined_img.jpg\n",
            "data/kornbredteev/joined_img.jpg\n",
            "data/sazwan_95/joined_img.jpg\n",
            "data/mulia.budy/joined_img.jpg\n",
            "data/staiy55/joined_img.jpg\n",
            "data/namasayava/joined_img.jpg\n",
            "data/dinsy99/joined_img.jpg\n",
            "data/setyanto2706/joined_img.jpg\n",
            "data/milfs.indian/joined_img.jpg\n",
            "data/hot_dilf_feet/joined_img.jpg\n",
            "data/bigd.ick4623/joined_img.jpg\n",
            "data/bigd.ick4629/joined_img.jpg\n",
            "data/lamaer508/joined_img.jpg\n",
            "data/haruy.r/joined_img.jpg\n",
            "data/antonellasofiamoreno/joined_img.jpg\n",
            "data/hungredy_3/joined_img.jpg\n",
            "data/23.1osohee/joined_img.jpg\n",
            "data/krishnashyammishra28/joined_img.jpg\n",
            "data/likeeebutter/joined_img.jpg\n",
            "data/lyingisland/joined_img.jpg\n",
            "data/lover.feedzzz/joined_img.jpg\n",
            "data/follow_tanveer.gilll/joined_img.jpg\n",
            "data/numbaa11/joined_img.jpg\n",
            "data/m.a.d.d.i.2005/joined_img.jpg\n",
            "data/big.nng/joined_img.jpg\n",
            "data/ch.ristian5363/joined_img.jpg\n",
            "data/acjaiajsksl/joined_img.jpg\n",
            "data/9062.sani/joined_img.jpg\n",
            "data/irah180605gmail.com8/joined_img.jpg\n",
            "data/nrmynti0824/joined_img.jpg\n",
            "data/johnjames13051990/joined_img.jpg\n",
            "data/acjaiajsksl/joined_img.jpg\n",
            "data/his.mayanti/joined_img.jpg\n",
            "data/aussy_bakkes/joined_img.jpg\n",
            "data/yoneiby0__0/joined_img.jpg\n",
            "data/sivha.sing/joined_img.jpg\n",
            "data/5julissa_300_juliana_vx/joined_img.jpg\n",
            "data/mindfulness.kite/joined_img.jpg\n",
            "data/syaripah_we/joined_img.jpg\n",
            "data/johnk939620/joined_img.jpg\n",
            "data/dinidinii40/joined_img.jpg\n",
            "data/marwotoarlan/joined_img.jpg\n",
            "data/jozsi0918273645/joined_img.jpg\n",
            "data/alexis_chose/joined_img.jpg\n",
            "data/action_ou_veriter_hot/joined_img.jpg\n",
            "data/ambreychaney_/joined_img.jpg\n",
            "data/neymarjunior2.0vibes/joined_img.jpg\n",
            "data/liza_lyz245/joined_img.jpg\n",
            "data/hofdefender/joined_img.jpg\n",
            "data/georgepeters969/joined_img.jpg\n",
            "data/onlyn351/joined_img.jpg\n",
            "data/nkheleuf/joined_img.jpg\n",
            "data/ilbalnugraha/joined_img.jpg\n",
            "data/ritvakorhonen2023/joined_img.jpg\n",
            "data/ayang_angga84/joined_img.jpg\n",
            "data/peju.ang4080/joined_img.jpg\n",
            "data/chell6194/joined_img.jpg\n",
            "data/awanputra902gmail.com7/joined_img.jpg\n",
            "data/87benjohnson/joined_img.jpg\n",
            "data/jokersson7/joined_img.jpg\n",
            "data/slipsnfallslive/joined_img.jpg\n",
            "data/sirach51k51/joined_img.jpg\n",
            "data/schei_dflschiedsrichter/joined_img.jpg\n",
            "data/shell.o.kk26/joined_img.jpg\n",
            "data/callmebabe8889/joined_img.jpg\n",
            "data/yxgfdq/joined_img.jpg\n",
            "data/meejks/joined_img.jpg\n",
            "data/sfpfsr/joined_img.jpg\n",
            "data/bbukico/joined_img.jpg\n",
            "data/cppijazc/joined_img.jpg\n",
            "data/kcqaw2024/joined_img.jpg\n",
            "data/payxc2024/joined_img.jpg\n",
            "data/ewsfkrd/joined_img.jpg\n",
            "data/phoovow/joined_img.jpg\n"
          ]
        }
      ],
      "source": [
        "#tensorflow pfp_classifier set-up p2\n",
        "import tensorflow as tf\n",
        "\n",
        "# Define a function to load and preprocess images\n",
        "def load_and_preprocess_image(path):\n",
        "    # Read the image file\n",
        "    print(path[\"path\"])\n",
        "    image = tf.io.read_file(path[\"path\"])\n",
        "    # Decode the image\n",
        "    image = tf.image.decode_jpeg(image, channels=3)\n",
        "    # Resize the image to a fixed size (e.g., 224x224)\n",
        "    image = tf.image.resize(image, [224, 224])\n",
        "    # Normalize the image to [0, 1] range\n",
        "    image = image / 255.0\n",
        "    return dict(image = image)\n",
        "\n",
        "# Apply the function to the dataset\n",
        "dataset = dict(path = pfps, label = lbls)\n",
        "post_dataset = dict(path = post_pics, label = lbls)\n",
        "\n",
        "dataset = Dataset.from_dict(dataset)\n",
        "post_dataset = Dataset.from_dict(post_dataset)\n",
        "\n",
        "dataset = dataset.map(load_and_preprocess_image)\n",
        "post_dataset = post_dataset.map(load_and_preprocess_image)\n",
        "\n",
        "#dataset_size = len(pfps)\n",
        "#train_size = int(0.8 * dataset_size)  # 80% for training\n",
        "#val_size = dataset_size - train_size  # Remaining 20% for validation\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "image_train, image_test, image_label_train, image_label_test = train_test_split(dataset[\"image\"], dataset[\"label\"], test_size=0.2, random_state=42)\n",
        "post_train, post_test, post_label_train, post_label_test = train_test_split(post_dataset[\"image\"], post_dataset[\"label\"], test_size=0.2, random_state=42)\n",
        "# Split the dataset\n",
        "#train_dataset = dataset.take(train_size)\n",
        "#val_dataset = dataset.skip(train_size)\n",
        "\n",
        "# Shuffle, batch, and prefetch the datasets\n",
        "#train_dataset = train_dataset.shuffle(buffer_size=1000).batch(32).prefetch(buffer_size=tf.data.experimental.AUTOTUNE)\n",
        "#val_dataset = val_dataset.batch(32).prefetch(buffer_size=tf.data.experimental.AUTOTUNE)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 60,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jpxW7YfyEKJO",
        "outputId": "897dc169-f033-4b4d-ca54-b3bf707d1ff6"
      },
      "outputs": [],
      "source": [
        "print(post_train[0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "sg7kD5tk-PvG"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Yippeee!\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "<tf_keras.src.callbacks.History at 0x26d98453710>"
            ]
          },
          "execution_count": 11,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.applications import ResNet50\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.layers import Dense, GlobalAveragePooling2D\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "\n",
        "\n",
        "#RESNET-50 model pre-trained and fine-tunable\n",
        "image_inputs = Input(shape=(224,224,3))\n",
        "base_model = tf.keras.applications.ResNet50(weights='imagenet', include_top=False, input_shape=(224, 224, 3))\n",
        "\n",
        "for layer in base_model.layers:\n",
        "    layer.trainable = False\n",
        "\n",
        "for layer in base_model.layers[-10:]:  # Adjust the number of layers to unfreeze as needed\n",
        "    layer.trainable = True\n",
        "\n",
        "x = base_model.output\n",
        "x = GlobalAveragePooling2D()(x)\n",
        "x = Dense(1024, activation='relu')(x)  # Example of a fully connected layer\n",
        "predictions = Dense(1, activation='softmax')(x)  # Change `num_classes` to your number of classes\n",
        "\n",
        "pfp_model = Model(inputs=base_model.input, outputs=predictions)\n",
        "pfp_model.compile(optimizer=Adam(learning_rate=0.001), loss='binary_crossentropy', metrics=['accuracy'])\n",
        "pfp_model.fit(image_train, image_label_train, epochs=10, verbose=0)\n",
        "\n",
        "\n",
        "post_model = Model(inputs=base_model.input, outputs=predictions)\n",
        "post_model.compile(optimizer=Adam(learning_rate=0.001), loss='binary_crossentropy', metrics=['accuracy'])\n",
        "post_model.fit(post_train, post_label_train, epochs=10, verbose=0)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_pyXlIBlhka1"
      },
      "source": [
        "####Text Classification:\n",
        "\n",
        "*   Usernames\n",
        "*   Biography\n",
        "*   Post Captions\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qb1Yhmvehrsz",
        "outputId": "a4fce5b8-ddce-48e7-f7f8-e00685e317a1"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "C:\\Users\\Aaron Nguyen\\AppData\\Local\\Temp\\ipykernel_21440\\2925406822.py:76: UserWarning: Pandas doesn't allow columns to be created via a new attribute name - see https://pandas.pydata.org/pandas-docs/stable/indexing.html#attribute-access\n",
            "  tokenized_train_dataset.col = [\"text\"]\n",
            "C:\\Users\\Aaron Nguyen\\AppData\\Local\\Temp\\ipykernel_21440\\2925406822.py:78: UserWarning: Pandas doesn't allow columns to be created via a new attribute name - see https://pandas.pydata.org/pandas-docs/stable/indexing.html#attribute-access\n",
            "  tokenized_val_dataset.col = [\"text\"]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From c:\\Users\\Aaron Nguyen\\anaconda3\\Lib\\site-packages\\tf_keras\\src\\optimizers\\__init__.py:317: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
            "\n",
            "Epoch 1/10\n",
            "5/5 [==============================] - 3s 194ms/step - loss: 0.6625 - accuracy: 0.7068\n",
            "Epoch 2/10\n",
            "5/5 [==============================] - 1s 193ms/step - loss: 0.5513 - accuracy: 0.8872\n",
            "Epoch 3/10\n",
            "5/5 [==============================] - 1s 194ms/step - loss: 0.4172 - accuracy: 0.9098\n",
            "Epoch 4/10\n",
            "5/5 [==============================] - 1s 196ms/step - loss: 0.2875 - accuracy: 0.9098\n",
            "Epoch 5/10\n",
            "5/5 [==============================] - 1s 190ms/step - loss: 0.2445 - accuracy: 0.9173\n",
            "Epoch 6/10\n",
            "5/5 [==============================] - 1s 189ms/step - loss: 0.2420 - accuracy: 0.9398\n",
            "Epoch 7/10\n",
            "5/5 [==============================] - 1s 191ms/step - loss: 0.2016 - accuracy: 0.9398\n",
            "Epoch 8/10\n",
            "5/5 [==============================] - 1s 189ms/step - loss: 0.2317 - accuracy: 0.9398\n",
            "Epoch 9/10\n",
            "5/5 [==============================] - 1s 188ms/step - loss: 0.1845 - accuracy: 0.9398\n",
            "Epoch 10/10\n",
            "5/5 [==============================] - 1s 192ms/step - loss: 0.2018 - accuracy: 0.9474\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "<tf_keras.src.callbacks.History at 0x26ded0bc050>"
            ]
          },
          "execution_count": 12,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "#ADD FOLLOWING:\n",
        "#Edit tokenization\n",
        "#Create train, and dev set\n",
        "def list_to_dict(data, labeled):\n",
        "    sentences1 = []\n",
        "    sentences2 = []\n",
        "    sentences3 = []\n",
        "    labels = []\n",
        "    for item in data:\n",
        "        sentences1.append(item[0])\n",
        "        sentences2.append(item[3])\n",
        "        sentences3.append(item[6])\n",
        "        if labeled:\n",
        "            labels.append(item[7])\n",
        "    if labeled:\n",
        "        convert_dict = dict(sentence1 = sentences1, sentence2 = sentences2, sentence3 = sentences3, label = labels)\n",
        "    else:\n",
        "        convert_dict = dict(sentence1 = sentences1, sentence2 = sentences2, sentences3 = sentences3)\n",
        "    return convert_dict\n",
        "\n",
        "\n",
        "#Tokenizes the dataset, by creating the input_id from tokenizing the two sentences\n",
        "def tokenize_function(dataset):\n",
        "    captionlist = []\n",
        "    for captions in range(0,len(dataset[\"sentence1\"])):\n",
        "        captionlist.append(dataset[\"sentence1\"][captions] + \"[SEP]\")\n",
        "    for captions in range(0,len(dataset[\"sentence2\"])):\n",
        "        captionlist[captions] += dataset[\"sentence2\"][captions] + \"[SEP]\"\n",
        "    for captions in range(0,len(dataset[\"sentence3\"])):\n",
        "        tempcaptions = \"\"\n",
        "        for caption in dataset[\"sentence3\"][captions]:\n",
        "            tempcaptions += caption\n",
        "        captionlist[captions] += tempcaptions\n",
        "    #Change to usernames [0], biography [3], post captions [6]\n",
        "    return captionlist\n",
        "    return dict(tokenizer(dataset[\"sentence1\"], dataset[\"sentence2\"], captionlist, truncation=True, padding=True, return_tensors=\"np\"))\n",
        "\n",
        "\n",
        "total_set = Dataset.from_dict(list_to_dict(train_data, True))\n",
        "labels = total_set[\"label\"]\n",
        "\n",
        "max_features = 200000\n",
        "embedding_dim = 128\n",
        "sequence_length = 500\n",
        "\n",
        "vectorize_layer = TextVectorization(\n",
        "    max_tokens = max_features,\n",
        "    output_mode = \"int\",\n",
        "    output_sequence_length = sequence_length\n",
        ")\n",
        "\n",
        "\n",
        "text_train, text_val, label_train, label_val = train_test_split(total_set, labels, test_size=0.2, random_state=42)\n",
        "\n",
        "\n",
        "#text_train = text_train.map(vectorize_text)\n",
        "\n",
        "text_inputs = Input(shape=(1,), dtype=\"string\")\n",
        "x = vectorize_layer(text_inputs)\n",
        "x = Embedding(max_features, embedding_dim)(x)\n",
        "x = Dropout(0.5)(x)\n",
        "x = Conv1D(128, 7, padding = \"valid\", activation = \"relu\", strides = 3) (x)\n",
        "x = Conv1D(128, 7, padding = \"valid\", activation = \"relu\", strides = 3) (x)\n",
        "x = GlobalMaxPooling1D() (x)\n",
        "x = Dense(128, activation = \"relu\")(x)\n",
        "x = Dropout(0.5)(x)\n",
        "outputs = Dense(1, activation = \"sigmoid\", name = \"outputs\")(x)\n",
        "text_model = Model(text_inputs, outputs)\n",
        "#Finetune RoBERTa large with the training data\n",
        "#text_model = TFAutoModelForSequenceClassification.from_pretrained(\"FacebookAI/roberta-large\", num_labels=2)\n",
        "#tokenizer = AutoTokenizer.from_pretrained(\"FacebookAI/roberta-large\")\n",
        "\n",
        "#Turn training, dev, and test set into datasets\n",
        "#Tokenize the datasets for processing\n",
        "tokenized_train_dataset = pd.DataFrame(data = tokenize_function(text_train))\n",
        "tokenized_train_dataset.col = [\"text\"]\n",
        "tokenized_val_dataset = pd.DataFrame(data = tokenize_function(text_val))\n",
        "tokenized_val_dataset.col = [\"text\"]\n",
        "vectorize_layer.adapt(tokenized_train_dataset)\n",
        "#tokenized_dev_dataset = dev_dataset.map(tokenize_function, batched=True)\n",
        "\n",
        "text_model.compile(optimizer=\"adam\", metrics=[\"accuracy\"], loss = \"binary_crossentropy\")  # No loss argument!\n",
        "\n",
        "text_model.fit([tokenized_train_dataset], y_train, epochs=10)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HVQZ96o6h2_y"
      },
      "source": [
        "####Multimodal Neural Network Model\n",
        "\n",
        "Combine all features and feed through single model\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "GbCGWzFDh-ty"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/20\n",
            "17/17 - 33s - loss: 0.6521 - accuracy: 0.5263 - precision_5: 0.0000e+00 - recall_5: 0.0000e+00 - 33s/epoch - 2s/step\n",
            "Epoch 2/20\n",
            "17/17 - 10s - loss: 0.6320 - accuracy: 0.5263 - precision_5: 0.0000e+00 - recall_5: 0.0000e+00 - 10s/epoch - 587ms/step\n",
            "Epoch 3/20\n",
            "17/17 - 10s - loss: 0.6200 - accuracy: 0.5263 - precision_5: 0.0000e+00 - recall_5: 0.0000e+00 - 10s/epoch - 590ms/step\n",
            "Epoch 4/20\n",
            "17/17 - 11s - loss: 0.6133 - accuracy: 0.6316 - precision_5: 1.0000 - recall_5: 0.2222 - 11s/epoch - 622ms/step\n",
            "Epoch 5/20\n",
            "17/17 - 10s - loss: 0.6069 - accuracy: 0.9549 - precision_5: 0.9130 - recall_5: 1.0000 - 10s/epoch - 614ms/step\n",
            "Epoch 6/20\n",
            "17/17 - 11s - loss: 0.6010 - accuracy: 0.9549 - precision_5: 0.9130 - recall_5: 1.0000 - 11s/epoch - 638ms/step\n",
            "Epoch 7/20\n",
            "17/17 - 11s - loss: 0.5944 - accuracy: 0.9549 - precision_5: 0.9130 - recall_5: 1.0000 - 11s/epoch - 633ms/step\n",
            "Epoch 8/20\n",
            "17/17 - 11s - loss: 0.5881 - accuracy: 0.9549 - precision_5: 0.9130 - recall_5: 1.0000 - 11s/epoch - 633ms/step\n",
            "Epoch 9/20\n",
            "17/17 - 10s - loss: 0.5813 - accuracy: 0.9549 - precision_5: 0.9130 - recall_5: 1.0000 - 10s/epoch - 598ms/step\n",
            "Epoch 10/20\n",
            "17/17 - 10s - loss: 0.5745 - accuracy: 0.9549 - precision_5: 0.9130 - recall_5: 1.0000 - 10s/epoch - 606ms/step\n",
            "Epoch 11/20\n",
            "17/17 - 10s - loss: 0.5676 - accuracy: 0.9549 - precision_5: 0.9130 - recall_5: 1.0000 - 10s/epoch - 599ms/step\n",
            "Epoch 12/20\n",
            "17/17 - 10s - loss: 0.5606 - accuracy: 0.9549 - precision_5: 0.9130 - recall_5: 1.0000 - 10s/epoch - 575ms/step\n",
            "Epoch 13/20\n",
            "17/17 - 10s - loss: 0.5537 - accuracy: 0.9549 - precision_5: 0.9130 - recall_5: 1.0000 - 10s/epoch - 599ms/step\n",
            "Epoch 14/20\n",
            "17/17 - 10s - loss: 0.5469 - accuracy: 0.9549 - precision_5: 0.9130 - recall_5: 1.0000 - 10s/epoch - 593ms/step\n",
            "Epoch 15/20\n",
            "17/17 - 10s - loss: 0.5393 - accuracy: 0.9549 - precision_5: 0.9130 - recall_5: 1.0000 - 10s/epoch - 602ms/step\n",
            "Epoch 16/20\n",
            "17/17 - 10s - loss: 0.5317 - accuracy: 0.9549 - precision_5: 0.9130 - recall_5: 1.0000 - 10s/epoch - 598ms/step\n",
            "Epoch 17/20\n",
            "17/17 - 11s - loss: 0.5239 - accuracy: 0.9549 - precision_5: 0.9130 - recall_5: 1.0000 - 11s/epoch - 635ms/step\n",
            "Epoch 18/20\n",
            "17/17 - 11s - loss: 0.5159 - accuracy: 0.9549 - precision_5: 0.9130 - recall_5: 1.0000 - 11s/epoch - 632ms/step\n",
            "Epoch 19/20\n",
            "17/17 - 11s - loss: 0.5080 - accuracy: 0.9549 - precision_5: 0.9130 - recall_5: 1.0000 - 11s/epoch - 624ms/step\n",
            "Epoch 20/20\n",
            "17/17 - 10s - loss: 0.5002 - accuracy: 0.9549 - precision_5: 0.9130 - recall_5: 1.0000 - 10s/epoch - 617ms/step\n",
            "WARNING:tensorflow:6 out of the last 11 calls to <function Model.make_test_function.<locals>.test_function at 0x0000026C3522CF40> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
            "2/2 - 12s - loss: 0.5168 - accuracy: 0.8824 - precision_5: 0.7778 - recall_5: 1.0000 - 12s/epoch - 6s/step\n",
            "Test Loss: 0.5168300867080688\n",
            "Test Accuracy: 0.8823529481887817\n",
            "f1-loss: 0.8750000083819031\n"
          ]
        }
      ],
      "source": [
        "post_inputs = Input(shape=(224,224,3))\n",
        "# concatenate all separate inputs and process\n",
        "combined = concatenate([num_model(num_input), text_model(text_inputs), pfp_model(image_inputs), post_model(post_inputs)])\n",
        "z = Dense(10, activation=\"relu\")(combined)\n",
        "z = Dense(1, activation=\"sigmoid\")(z)\n",
        "\n",
        "# final model\n",
        "model = Model(inputs=[num_input, text_inputs, image_inputs, post_inputs], outputs=z)\n",
        "\n",
        "image_train = numpy.asarray(image_train)\n",
        "image_test = numpy.asarray(image_test)\n",
        "post_train = numpy.asarray(post_train)\n",
        "post_test = numpy.asarray(post_test)\n",
        "\n",
        "# training\n",
        "model.compile(loss=\"binary_crossentropy\", metrics=[\"accuracy\", tf.keras.metrics.Precision(), tf.keras.metrics.Recall()])\n",
        "model.fit([x_num_train, tokenized_train_dataset, image_train, post_train], y_train, epochs=20, batch_size=8, verbose=2)\n",
        "\n",
        "# evaluate based on loss and accuracy\n",
        "loss, accuracy, precision, recall = model.evaluate([x_num_test, tokenized_val_dataset, image_test, post_test], y_test, verbose=2)\n",
        "f1 = 2* precision*recall/(precision + recall)\n",
        "print(f\"Test Loss: {loss}\")\n",
        "print(f\"Test Accuracy: {accuracy}\")\n",
        "print(f\"f1-loss: {f1}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YEqF4iZwFcbm"
      },
      "outputs": [],
      "source": [
        "# create prediction from model\n",
        "y_pred = model.predict([x_num_test, x_text_test])\n",
        "y_pred_classes = (y_pred > 0.5).astype(\"int32\")  # Convert probabilities to binary class predictions\n",
        "\n",
        "# print report\n",
        "print(classification_report(y_test, y_pred_classes))"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
